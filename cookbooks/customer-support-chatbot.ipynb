{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from mem0 import Memory\n",
    "from datetime import datetime\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"gpustack_b0af60452b6eb71c_e8e6a2a6c779934a140a74749ddfa5fd\"  # needed for embedding model\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"http://localhost/v1\"  # needed for embedding model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportChatbot:\n",
    "    def __init__(self):\n",
    "        # Initialize Mem0 with Anthropic's Claude\n",
    "        self.config = {\n",
    "            \"llm\": {\n",
    "                \"provider\": \"openai\",\n",
    "                \"config\": {\n",
    "                    \"model\": \"qwen3\",\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"max_tokens\": 2000,\n",
    "                },\n",
    "            },\n",
    "            \"embedder\": {\n",
    "                \"provider\": \"openai\",\n",
    "                \"config\": {\n",
    "                    \"model\": \"qwen3-embedding\",\n",
    "                    \"openai_base_url\": \"http://localhost/v1\",\n",
    "                    \"api_key\": \"gpustack_b0af60452b6eb71c_e8e6a2a6c779934a140a74749ddfa5fd\",\n",
    "                    \"embedding_dims\": 4096\n",
    "                },\n",
    "            },\n",
    "            \"vector_store\": {\n",
    "               \"provider\": \"qdrant\",\n",
    "               \"config\": { \n",
    "                   \"collection_name\": \"memories\",\n",
    "                   \"embedding_model_dims\": 4096\n",
    "               }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "            base_url=\"http://localhost/v1\"\n",
    "        ) \n",
    "        self.memory = Memory.from_config(self.config)\n",
    "\n",
    "        # Define support context\n",
    "        self.system_context = \"\"\"\n",
    "        You are a helpful customer support agent. Use the following guidelines:\n",
    "        - Be polite and professional\n",
    "        - Show empathy for customer issues\n",
    "        - Reference past interactions when relevant\n",
    "        - Maintain consistent information across conversations\n",
    "        - If you're unsure about something, ask for clarification\n",
    "        - Keep track of open issues and follow-ups\n",
    "        \"\"\"\n",
    "\n",
    "    def store_customer_interaction(self, user_id: str, message: str, response: str, metadata: Dict = None):\n",
    "        \"\"\"Store customer interaction in memory.\"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "\n",
    "        # Add timestamp to metadata\n",
    "        metadata[\"timestamp\"] = datetime.now().isoformat()\n",
    "\n",
    "        # Format conversation for storage\n",
    "        conversation = [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": response}]\n",
    "\n",
    "        # Store in Mem0\n",
    "        self.memory.add(conversation, user_id=user_id, metadata=metadata)\n",
    "\n",
    "    def get_relevant_history(self, user_id: str, query: str) -> List[Dict]:\n",
    "        \"\"\"Retrieve relevant past interactions.\"\"\"\n",
    "        return self.memory.search(\n",
    "            query=query,\n",
    "            user_id=user_id,\n",
    "            limit=5,  # Adjust based on needs\n",
    "        )\n",
    "\n",
    "    def handle_customer_query(self, user_id: str, query: str) -> str:\n",
    "        \"\"\"Process customer query with context from past interactions.\"\"\"\n",
    "\n",
    "        # Get relevant past interactions\n",
    "        relevant_history = self.get_relevant_history(user_id, query)\n",
    "\n",
    "        # Build context from relevant history\n",
    "        context = \"Previous relevant interactions:\\n\"\n",
    "        for memory in relevant_history:\n",
    "            try:\n",
    "                context += f\"Customer: {memory['memory']}\\n\"\n",
    "                context += f\"Support: {memory['memory']}\\n\"\n",
    "                context += \"---\\n\"\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "        # Prepare prompt with context and current query\n",
    "        prompt = f\"\"\"\n",
    "        {self.system_context}\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Current customer query: {query}\n",
    "\n",
    "        Provide a helpful response that takes into account any relevant past interactions.\n",
    "        \"\"\"\n",
    "        print(prompt)\n",
    "        # Generate response using Claude\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"qwen3\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=2000,\n",
    "            temperature=0.1,\n",
    "        )\n",
    "\n",
    "        # Store interaction\n",
    "        self.store_customer_interaction(\n",
    "            user_id=user_id, message=query, response=response.choices[0].message.content, metadata={\"type\": \"support_query\"}\n",
    "        ) \n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Customer Support! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ä½ å¥½\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer: ä½ å¥½\n",
      "\n",
      "        \n",
      "        You are a helpful customer support agent. Use the following guidelines:\n",
      "        - Be polite and professional\n",
      "        - Show empathy for customer issues\n",
      "        - Reference past interactions when relevant\n",
      "        - Maintain consistent information across conversations\n",
      "        - If you're unsure about something, ask for clarification\n",
      "        - Keep track of open issues and follow-ups\n",
      "        \n",
      "\n",
      "        Previous relevant interactions:\n",
      "\n",
      "\n",
      "        Current customer query: ä½ å¥½\n",
      "\n",
      "        Provide a helpful response that takes into account any relevant past interactions.\n",
      "        \n",
      "Support: \n",
      "\n",
      "æ‚¨å¥½ï¼å¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨è§£ç­”ã€‚ç¥æ‚¨ç”Ÿæ´»æ„‰å¿«ï¼ \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " æˆ‘å–œæ¬¢åƒè‰è“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer: æˆ‘å–œæ¬¢åƒè‰è“\n",
      "\n",
      "        \n",
      "        You are a helpful customer support agent. Use the following guidelines:\n",
      "        - Be polite and professional\n",
      "        - Show empathy for customer issues\n",
      "        - Reference past interactions when relevant\n",
      "        - Maintain consistent information across conversations\n",
      "        - If you're unsure about something, ask for clarification\n",
      "        - Keep track of open issues and follow-ups\n",
      "        \n",
      "\n",
      "        Previous relevant interactions:\n",
      "\n",
      "\n",
      "        Current customer query: æˆ‘å–œæ¬¢åƒè‰è“\n",
      "\n",
      "        Provide a helpful response that takes into account any relevant past interactions.\n",
      "        \n",
      "Support: \n",
      "\n",
      "I'm glad to hear that you like strawberries! If you have any questions or need help with anything related to strawberries, feel free to ask. I'm here to support you! ğŸ˜Š \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ä½ å–œæ¬¢ä»€ä¹ˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer: ä½ å–œæ¬¢ä»€ä¹ˆ\n",
      "\n",
      "        \n",
      "        You are a helpful customer support agent. Use the following guidelines:\n",
      "        - Be polite and professional\n",
      "        - Show empathy for customer issues\n",
      "        - Reference past interactions when relevant\n",
      "        - Maintain consistent information across conversations\n",
      "        - If you're unsure about something, ask for clarification\n",
      "        - Keep track of open issues and follow-ups\n",
      "        \n",
      "\n",
      "        Previous relevant interactions:\n",
      "\n",
      "\n",
      "        Current customer query: ä½ å–œæ¬¢ä»€ä¹ˆ\n",
      "\n",
      "        Provide a helpful response that takes into account any relevant past interactions.\n",
      "        \n",
      "Support: \n",
      "\n",
      "æ‚¨å¥½ï¼æ„Ÿè°¢æ‚¨çš„æé—®ï¼Œæˆ‘å¾ˆä¹æ„ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦æ”¯æŒï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨è§£ç­”ã€‚æœŸå¾…ä¸æ‚¨äº¤æµï¼ \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer: 1\n",
      "\n",
      "        \n",
      "        You are a helpful customer support agent. Use the following guidelines:\n",
      "        - Be polite and professional\n",
      "        - Show empathy for customer issues\n",
      "        - Reference past interactions when relevant\n",
      "        - Maintain consistent information across conversations\n",
      "        - If you're unsure about something, ask for clarification\n",
      "        - Keep track of open issues and follow-ups\n",
      "        \n",
      "\n",
      "        Previous relevant interactions:\n",
      "\n",
      "\n",
      "        Current customer query: 1\n",
      "\n",
      "        Provide a helpful response that takes into account any relevant past interactions.\n",
      "        \n",
      "Support: \n",
      "\n",
      "Hello! I'm here to assist you. Could you please share more details about the issue you're facing? For example, what exactly are you trying to accomplish, or what specific problem you're encountering? I'll do my best to resolve it. Let me know how I can help! \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " æˆ‘å–œæ¬¢ä»€ä¹ˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer: æˆ‘å–œæ¬¢ä»€ä¹ˆ\n",
      "\n",
      "        \n",
      "        You are a helpful customer support agent. Use the following guidelines:\n",
      "        - Be polite and professional\n",
      "        - Show empathy for customer issues\n",
      "        - Reference past interactions when relevant\n",
      "        - Maintain consistent information across conversations\n",
      "        - If you're unsure about something, ask for clarification\n",
      "        - Keep track of open issues and follow-ups\n",
      "        \n",
      "\n",
      "        Previous relevant interactions:\n",
      "\n",
      "\n",
      "        Current customer query: æˆ‘å–œæ¬¢ä»€ä¹ˆ\n",
      "\n",
      "        Provide a helpful response that takes into account any relevant past interactions.\n",
      "        \n",
      "Support: \n",
      "\n",
      "I'm happy to help! Could you please clarify what you mean by \"what\" you like? I'd be happy to support you further! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chatbot = SupportChatbot()\n",
    "user_id = \"customer_bot\"\n",
    "print(\"Welcome to Customer Support! Type 'exit' to end the conversation.\")\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    query = input()\n",
    "    print(\"Customer:\", query)\n",
    "\n",
    "    # Check if user wants to exit\n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"Thank you for using our support service. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Handle the query and print the response\n",
    "    response = chatbot.handle_customer_query(user_id, query)\n",
    "    print(\"Support:\", response, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "    base_url=\"http://localhost/v1\"\n",
    ") \n",
    "\n",
    "# Generate response using Claude\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen3\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"ä½ å¥½\"}],\n",
    "    max_tokens=2000,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mem0] *",
   "language": "python",
   "name": "conda-env-mem0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
